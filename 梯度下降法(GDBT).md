
在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。

## 概念

### 梯度
在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数$f(x,y)$, 分别对$x$,$y$求偏导数，求得的梯度向量就是$(∂f/∂x, ∂f/∂y)^T$,简称$grad f(x,y)$或者$▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$(∂f/∂x_0, ∂f/∂y_0)^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是$(∂f/∂x, ∂f/∂y，∂f/∂z)^T$,以此类推。

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数$f(x,y)$,在点$(x_0,y_0)$，沿着梯度向量的方向就是$(∂f/∂x_0, ∂f/∂y_0)^T$的方向是$f(x,y)$增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是$-(∂f/∂x_0, ∂f/∂y_0)^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。
    
### 步长（Learning rate）
步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。

### 特征（feature）
：指的是样本中输入部分，比如样本$(x_0,y_0)$,$(x_1,y_1)$,则样本特征为$X$，样本输出为$y$。

### 假设函数（hypothesis function）
在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h\_θ(x)$。比如对于样本$(x_i,y_i) (i=1,2,...n)$,可以采用拟合函数如下：$h\_θ(x) = θ\_0+θ\_1x$。

### 损失函数（loss function）

为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本$(x_i,y_i) (i=1,2,...n)$,采用线性回归，损失函数为：$J(θ\_0,θ\_1)=∑i=1m(hθ(xi)−yi)^2$，其中xi表示样本特征x的第i个元素，yi表示样本输出y的第i个元素，hθ(xi)为假设函数。 

## 



```{java, echo=FALSE}
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
```
